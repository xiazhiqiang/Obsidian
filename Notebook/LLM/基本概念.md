## LM
大模型是指具有大量参数和复杂结构的机器学习模型，能够处理海量数据、完成各种复杂的任务。
按照应用领域划分：
- 通用大模型L0：是指可以在多个领域和任务上通用的大模型。它们利用大算力、使用海量的开放数据与具有巨量参数的深度学习算法在大规模无标注数据上进行训练，以寻找特征并发现规律，进而形成可“举一反三"的强大泛化能力，可在不进行微调或少量微调的情况下完成多场景任务，相当于AI完成了“通识教育”。
- 行业大模型L1：是指那些针对特定行业或领域的大模型。它们通常使用行业相关的数据进行预训练或微调，以提高在该领域的性能和准确度，相当于AI成为“行业专家“。
- 垂直大模型L2：是指那些针对特定任务或场景的大模型。它们通常使用任务相关的数据进行预训练或微调，以提高在该任务上的性能和效果。

### 模型的泛化能力
是指一个模型在面对新的、未见过的数据时，能够正确理解和预测这些数据的能力。

## LLM
大语言模型（Large Language Model）通常是具有大规模参数和计算能力的自然语言处理模型。
这些模型可以通过大量的数据和参数进行训练，从而生成人类类似的文本或回答自然语言的问题。
大语言模型在自然语言处理、文本生成和智能对话等领域有广泛应用。

## 多模态大模型
多模态大模型是指能够处理多种不同类型数据的大模型，例如文本、图像、音频等多模态数据。这类模型结合了NLP和CV的能力，以实现对多模态信息的综合理解和分析，从而能够更全面地理解和处理复杂的数据。

## LLAMA
Large Language Model Meta AI 是 Meta 在2023年3月，发布并开源了一款新型大模型，其参数量范围从70亿至650亿。值得注意的是，参数量为130亿的LLaMA模型在大部分基准测试中的表现，已超越了参数量高达1750亿的GPT-3。并且，这款模型可以在单块V100 GPU上运行。

## GPT
生成式预训练转换器（Generative Pre-trained Transformer），旨在生成自然语言文本并处理各种自然语言处理任务，如文本生成、翻译、摘要等。它通常在单向生成的情况下使用，即根据给定的文本生成连贯的输出。

## PT
预训练（Pre-training）是一种无监督学习方法，模型通过大量无标签数据进行训练，以捕捉数据的底层结构和模式。预训练的目的是让模型学会一定程度的通用知识，为后续的微调阶段打下基础。

## FT
微调（Fine-tuning）是通过在预训练模型的最后一层添加一个新的分类层，然后根据新的数据集进行微调。
相对于从头开始训练(Training a modelfrom scatch)，微调可以省去大量计算资源和计算时间，提高计算效率，甚至提高准确率。

## SFT
监督微调（Supervised fine-tuning）是一种有监督学习方法，通过在有标签数据上对预训练模型进行进一步训练，以适应特定的任务。这个过程使得模型能够利用预训练阶段学到的通用知识，结合新数据的标签信息，使模型在特定任务上表现更好。

## Embedding
是一种将离散型变量（如单词、商品、用户等）映射到连续的向量空间中的方法，这种向量通常被称为"embedding vector"或"embedding representation"。这种映射方法可以捕捉到变量之间的相似性和关系。
嵌入式方法是一种可以将高维的离散型数据（如单词、商品、用户等）映射到低维的连续向量空间的方法，这种映射方法可以捕捉到变量之间的相似性和关系，从而方便进行机器学习和深度学习的处理。

## Token
大模型 Token 是指模型处理的输入文本中的单词、标点符号或其他文本单元等。在自然语言处理任务中，文本通常会被分解成一个个token，以便模型理解文本的含义和结构，这些 tokens 被转换成向量形式输入到模型中进行训练和推理。



